{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Users\\Vishwesh\\Anaconda3\\envs\\deep_l\\lib\\site-packages\\sklearn\\cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from scipy.io import loadmat, savemat\n",
    "import numpy as np\n",
    "import time\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn import metrics\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn import cross_validation\n",
    "\n",
    "from keras.backend.tensorflow_backend import set_session\n",
    "import tensorflow as tf\n",
    "from keras import backend as K\n",
    "\n",
    "from keras.layers import Input, Dense, Dropout, merge, concatenate, Convolution3D, Flatten\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.core import Lambda\n",
    "from keras.models import Model\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Activation\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "\n",
    "from keras.optimizers import SGD, adam, nadam, Adagrad, RMSprop\n",
    "from keras.regularizers import l1,l2\n",
    "\n",
    "from keras.callbacks import EarlyStopping, CSVLogger\n",
    "\n",
    "\n",
    "import os\n",
    "import os.path\n",
    "import sys\n",
    "import argparse\n",
    "import time\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/cpu:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 12523498828434716435\n",
      ", name: \"/gpu:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 5080789811\n",
      "locality {\n",
      "  bus_id: 1\n",
      "}\n",
      "incarnation: 13176037114117543784\n",
      "physical_device_desc: \"device: 0, name: GeForce GTX 970M, pci bus id: 0000:01:00.0\"\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def euclidean_distance(vects):\n",
    "    x, y, z = vects\n",
    "    #euc_dist = K.sqrt(K.sum(K.square(x - y), axis=1, keepdims=True))    \n",
    "    #euc_dist = K.square(x - y)\n",
    "    euc_dist = K.square(x - y)\n",
    "    out = euc_dist + z\n",
    "    #print(euc_dist)\n",
    "    #print(out)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def identity_loss(y_true, y_pred):\n",
    "    #loss_1 = K.cast(loss_1, dtype='float64')\n",
    "    \n",
    "    # Extract aux1, aux2 and main o/p\n",
    "    loss_1 = K.mean(K.square(y_pred[:,:66]-y_pred[:,66:132]))\n",
    "    loss_2 = K.mean(K.square(y_pred[:,132:]-y_true))\n",
    "    loss = loss_1 + loss_2\n",
    "    #loss_1 = K.mean(y_true - y_pred)\n",
    "    #return K.mean(y_pred - 0 * y_true)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_cnn_network(input_dim):\n",
    "    \n",
    "    seq = Sequential()\n",
    "    \n",
    "    seq.add(Dense(45, input_shape=(input_dim,)))\n",
    "    #seq.add(BatchNormalization())\n",
    "    \n",
    "    seq.add(Dense(400))\n",
    "    seq.add(Activation(\"relu\"))\n",
    "    #seq.add(BatchNormalization())\n",
    "    \n",
    "    seq.add(Dense(66))\n",
    "    seq.add(Activation(\"relu\"))\n",
    "    #seq.add(BatchNormalization())\n",
    "    \n",
    "    seq.add(Dense(200))\n",
    "    #seq.add(BatchNormalization())\n",
    "    \n",
    "    # Output Layer.\n",
    "    seq.add(Dense(66))\n",
    "    print(seq.summary())\n",
    "    return seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_anchor():\n",
    "    # Load all data\n",
    "    ip = loadmat('train_anchor_input.mat')\n",
    "    main_X = np.array(ip['train_anchor_input'])\n",
    "    \n",
    "    output = loadmat('train_anchor_output.mat')\n",
    "    y = np.array(output['train_anchor_output'])\n",
    "    \n",
    "    # Get dimensions of arrays\n",
    "    x_size = main_X.shape\n",
    "    print('Input Array Shape',x_size)\n",
    "    y_size = y.shape\n",
    "    print ('Output Array Shape',y_size)\n",
    "    \n",
    "    return main_X,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_aux_data():\n",
    "    aux_1 = loadmat('ta_voxels.mat')\n",
    "    aux_2 = loadmat('tb_voxels.mat')\n",
    "\n",
    "    aux_1_ip = np.array(aux_1['ta_voxels'])\n",
    "    aux_2_ip = np.array(aux_2['tb_voxels'])\n",
    "\n",
    "    aux_1_size = aux_1_ip.shape\n",
    "    print ('Aux 1 Array Shape',aux_1_size)\n",
    "    aux_2_size = aux_2_ip.shape\n",
    "    print ('Aux 2 Array Shape',aux_2_size)\n",
    "    \n",
    "    return aux_1_ip,aux_2_ip\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_nsnet():\n",
    "    input_dim = 45\n",
    "    # Three inputs\n",
    "    input_a = Input(shape=(input_dim,))\n",
    "    input_b = Input(shape=(input_dim,))\n",
    "    main_ip = Input(shape=(input_dim,))\n",
    "    \n",
    "    # Create the base net structure\n",
    "    base_network = create_cnn_network(input_dim)\n",
    "\n",
    "    # Feed the inputs to the base network\n",
    "    processed_a = base_network(input_a)\n",
    "    processed_b = base_network(input_b)\n",
    "    processed_main = base_network(main_ip)\n",
    "    \n",
    "    # Combine the pairwise voxels with the histo voxel.\n",
    "    #distance = Lambda(euclidean_distance)([processed_a, processed_b, processed_main])\n",
    "    concat_layer = concatenate([processed_a,processed_b,processed_main])\n",
    "    model = Model(input=[input_a, input_b, main_ip], output=concat_layer)\n",
    "    \n",
    "    opt_func = RMSprop()\n",
    "    #model.compile(loss='mse', optimizer=opt_func)\n",
    "    model.compile(loss=identity_loss, optimizer=opt_func)\n",
    "    print(model.summary())\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_nsnet(model, aux_1_ip, aux_2_ip, main_X, y, out_dir, n_epoch, val_size):\n",
    "    if not os.path.exists(out_dir):\n",
    "        os.makedirs(out_dir)\n",
    "    csv_logger = CSVLogger(os.path.join(out_dir, 'results.csv'))\n",
    "    model.fit([aux_1_ip, aux_2_ip, main_X], y, epochs=n_epoch, batch_size=100, verbose=1, shuffle=True, validation_split=val_size, callbacks=[csv_logger])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def save_estimate(model, aux_1_ip, aux_2_ip, X, y, out_file, indices):\n",
    "        \n",
    "    pred = model.predict([aux_1_ip, aux_2_ip, X])\n",
    "    \n",
    "    output_test = loadmat('test_set_output_10th_order_final.mat')\n",
    "    print(pred.shape)\n",
    "    \n",
    "    out_path = os.path.dirname(out_file)\n",
    "    if not os.path.exists(out_path):\n",
    "        os.makedirs(out_path)\n",
    "     \n",
    "    savemat(out_file, mdict={'out_pred': pred, 'out_true': y})\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def save_estimate_blind_hist(model, out_file):\n",
    "        \n",
    "    # Load the testing data\n",
    "    input_test = loadmat('test_set_input_b6000.mat')\n",
    "    input_dummy_a = loadmat('dummy_a.mat')\n",
    "    input_dummy_b = loadmat('dummy_b.mat')\n",
    "    \n",
    "    # Make numpy arrays\n",
    "    X_f_t = np.array(input_test['test_set_input_b6000'])\n",
    "    X_f_a = np.array(input_dummy_a['dummy_a'])\n",
    "    X_f_b = np.array(input_dummy_b['dummy_b'])\n",
    "    \n",
    "    pred = model.predict([X_f_a, X_f_b, X_f_t])\n",
    "    \n",
    "    output_test = loadmat('test_set_output_10th_order_final.mat')\n",
    "    y_f_t = np.array(output_test['test_set_output_10th_order_final'])\n",
    "    print(pred.shape)\n",
    "    \n",
    "    out_path = os.path.dirname(out_file)\n",
    "    if not os.path.exists(out_path):\n",
    "        os.makedirs(out_path)\n",
    "     \n",
    "    savemat(out_file, mdict={'out_pred': pred, 'out_true': y_f_t})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def save_estimate_ts04(model, out_file_1, out_file_2):\n",
    "    # In vivo test\n",
    "    ts4_3ta = loadmat('sh_ts04_3ta_feed.mat')\n",
    "    ts4_3tb = loadmat('sh_ts04_3tb_feed.mat')\n",
    "\n",
    "    # Make numpy arrays\n",
    "    X_3ta = np.array(ts4_3ta['sh_ts04_3ta_feed'])\n",
    "    X_3tb = np.array(ts4_3tb['sh_ts04_3tb_feed'])\n",
    "\n",
    "    input_dummy_a = loadmat('in_vivo_dummy_a.mat')\n",
    "    input_dummy_b = loadmat('in_vivo_dummy_b.mat')\n",
    "\n",
    "    X_f_a = np.array(input_dummy_a['dummy_a'])\n",
    "    X_f_b = np.array(input_dummy_b['dummy_b'])\n",
    "    \n",
    "    \n",
    "    out_path = os.path.dirname(out_file_1)\n",
    "    if not os.path.exists(out_path):\n",
    "        os.makedirs(out_path)\n",
    "    \n",
    "    # Pred 3TA TS04\n",
    "    pred = model.predict([X_f_a,X_f_b,X_3ta])\n",
    "    savemat(out_file_1, mdict={'predicted':pred})\n",
    "    print('TS04 f1 Saved')\n",
    "    \n",
    "    # Pred 3TB TS04\n",
    "    predi = model.predict([X_f_a,X_f_b,X_3tb])\n",
    "    savemat(out_file_2, mdict={'predicted':predi})\n",
    "    print('TS04 f2 Saved')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def save_estimate_ts01(model, out_file_1, out_file_2):\n",
    "    # In vivo test\n",
    "    ts4_3ta = loadmat('sh_ts01_3tb_feed.mat')\n",
    "    ts4_3tb = loadmat('sh_ts01_austin_feed.mat')\n",
    "\n",
    "    # Make numpy arrays\n",
    "    X_3ta = np.array(ts4_3ta['sh_ts01_3tb_feed'])\n",
    "    X_3tb = np.array(ts4_3tb['sh_ts01_austin_feed'])\n",
    "\n",
    "    input_dummy_a = loadmat('in_vivo_dummy_ts01_a.mat')\n",
    "    input_dummy_b = loadmat('in_vivo_dummy_ts01_b.mat')\n",
    "\n",
    "    X_f_a = np.array(input_dummy_a['dummy_a'])\n",
    "    X_f_b = np.array(input_dummy_b['dummy_b'])\n",
    "    \n",
    "    out_path = os.path.dirname(out_file_1)\n",
    "    if not os.path.exists(out_path):\n",
    "        os.makedirs(out_path)\n",
    "    \n",
    "    # Pred 3TB TS01\n",
    "    pred = model.predict([X_f_a,X_f_b,X_3ta])\n",
    "    savemat(out_file_1, mdict={'predicted':pred})\n",
    "    print('TS01 f1 Saved')\n",
    "    \n",
    "    # Pred Austin TS01\n",
    "    predi = model.predict([X_f_a,X_f_b,X_3tb])\n",
    "    savemat(out_file_2, mdict={'predicted':predi})\n",
    "    print('TS01 f2 Saved')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "    work_dir = os.getcwd()\n",
    "    exp = 'Null_space_CV_MICCAI_extended_loss_fixed_final'\n",
    "    itr = 3 # Estimated from CV\n",
    "    \n",
    "    print('Loading Data ...')\n",
    "    X, y = load_anchor()\n",
    "    indices = np.array(range(X.shape[0]))+1\n",
    "    \n",
    "    aux_1_ip, aux_2_ip = load_aux_data()\n",
    "    \n",
    "    out_start_dir = os.path.join(work_dir,exp)\n",
    "    seed1 = 46\n",
    "    \n",
    "    kf = KFold(n_splits=5, random_state=seed1,shuffle=True)\n",
    "    \n",
    "    model_D = build_nsnet()\n",
    "    fold_num = 0\n",
    "    \n",
    "    # Size of split for training and validation\n",
    "    val_size=0.2\n",
    "    \n",
    "    for train, test in kf.split(X):\n",
    "        fold_num += 1\n",
    "        X_train = X[train,:]\n",
    "        aux_1_ip_train = aux_1_ip[train,:]\n",
    "        aux_2_ip_train = aux_2_ip[train,:]\n",
    "        y_train = y[train,:]\n",
    "        \n",
    "        X_test = X[test,:]\n",
    "        aux_1_ip_test = aux_1_ip[test,:]\n",
    "        aux_2_ip_test = aux_2_ip[test,:]\n",
    "        y_test = y[test,:]\n",
    "        \n",
    "        \n",
    "        \n",
    "        indices_train = indices[train]\n",
    "        indices_test = indices[test]\n",
    "        \n",
    "        print (\"Training DNN with %d iterations, fold %d\" % (itr, fold_num))\n",
    "        out_dir_DNN = os.path.join(out_start_dir, str(fold_num))\n",
    "        model_D = train_nsnet(model_D, aux_1_ip_train, aux_2_ip_train, X_train, y_train, out_dir_DNN, itr, val_size)\n",
    "        \n",
    "        print (\"Saving training outputs\")\n",
    "        end_dir = os.path.join(out_start_dir, str(fold_num), 'training.csv')\n",
    "        # Set all aux_ip_train while predictiing to zeros\n",
    "        aux_1_ip_train.fill(0)\n",
    "        aux_2_ip_train.fill(0)\n",
    "        save_estimate(model_D, aux_1_ip_train, aux_2_ip_train, X_train, y_train, end_dir,indices_train)\n",
    "\n",
    "        print (\"Saving testing outputs\")\n",
    "        end_dir = os.path.join(out_start_dir, str(fold_num), 'testing.csv')\n",
    "        # Set all aux_ip_tests to zeros\n",
    "        aux_1_ip_test.fill(0)\n",
    "        aux_2_ip_test.fill(0)\n",
    "        save_estimate(model_D, aux_1_ip_test, aux_2_ip_test, X_test, y_test, end_dir,indices_test)\n",
    "    \n",
    "    print (\"Saving Histology blind test outputs\")\n",
    "    end_dir = os.path.join(out_start_dir, str('Hist_Blind_72_Test'), 'result_NSDN_b6000_testing_10th_order.mat')\n",
    "    save_estimate_blind_hist(model_D, end_dir)\n",
    "    \n",
    "    \n",
    "    print (\"Saving TS04 Reproducibility Results\")\n",
    "    end_dir_1 = os.path.join(out_start_dir, str('TS04'), 'result_NSDN_3ta.mat')\n",
    "    end_dir_2 = os.path.join(out_start_dir, str('TS04'), 'result_NSDN_3tb.mat')\n",
    "    save_estimate_ts04(model_D, end_dir_1, end_dir_2)\n",
    "    \n",
    "    print (\"Saving TS01 Reproducibility Results\")\n",
    "    end_dir_3 = os.path.join(out_start_dir, str('TS01'), 'result_NSDN_3tb.mat')\n",
    "    end_dir_4 = os.path.join(out_start_dir, str('TS01'), 'result_NSDN_austin.mat')\n",
    "    save_estimate_ts01(model_D, end_dir_3, end_dir_4)\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Data ...\n",
      "Input Array Shape (34679, 45)\n",
      "Output Array Shape (34679, 66)\n",
      "Aux 1 Array Shape (34679, 45)\n",
      "Aux 2 Array Shape (34679, 45)\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 45)                2070      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 400)               18400     \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 400)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 66)                26466     \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 66)                0         "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Users\\Vishwesh\\Anaconda3\\envs\\deep_l\\lib\\site-packages\\ipykernel\\__main__.py:19: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=[<tf.Tenso..., outputs=Tensor(\"co...)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 200)               13400     \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 66)                13266     \n",
      "=================================================================\n",
      "Total params: 73,602\n",
      "Trainable params: 73,602\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_1 (InputLayer)             (None, 45)            0                                            \n",
      "____________________________________________________________________________________________________\n",
      "input_2 (InputLayer)             (None, 45)            0                                            \n",
      "____________________________________________________________________________________________________\n",
      "input_3 (InputLayer)             (None, 45)            0                                            \n",
      "____________________________________________________________________________________________________\n",
      "sequential_1 (Sequential)        (None, 66)            73602       input_1[0][0]                    \n",
      "                                                                   input_2[0][0]                    \n",
      "                                                                   input_3[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)      (None, 198)           0           sequential_1[1][0]               \n",
      "                                                                   sequential_1[2][0]               \n",
      "                                                                   sequential_1[3][0]               \n",
      "====================================================================================================\n",
      "Total params: 73,602\n",
      "Trainable params: 73,602\n",
      "Non-trainable params: 0\n",
      "____________________________________________________________________________________________________\n",
      "None\n",
      "Training DNN with 3 iterations, fold 1\n",
      "Train on 22194 samples, validate on 5549 samples\n",
      "Epoch 1/3\n",
      "22194/22194 [==============================] - 2s - loss: 0.0078 - val_loss: 0.0061\n",
      "Epoch 2/3\n",
      "22194/22194 [==============================] - 1s - loss: 0.0056 - val_loss: 0.0061\n",
      "Epoch 3/3\n",
      "22194/22194 [==============================] - 1s - loss: 0.0052 - val_loss: 0.0061\n",
      "Saving training outputs\n",
      "(27743, 198)\n",
      "Saving testing outputs\n",
      "(6936, 198)\n",
      "Training DNN with 3 iterations, fold 2\n",
      "Train on 22194 samples, validate on 5549 samples\n",
      "Epoch 1/3\n",
      "22194/22194 [==============================] - 1s - loss: 0.0048 - val_loss: 0.0052\n",
      "Epoch 2/3\n",
      "22194/22194 [==============================] - 1s - loss: 0.0046 - val_loss: 0.0060\n",
      "Epoch 3/3\n",
      "22194/22194 [==============================] - 1s - loss: 0.0045 - val_loss: 0.0049\n",
      "Saving training outputs\n",
      "(27743, 198)\n",
      "Saving testing outputs\n",
      "(6936, 198)\n",
      "Training DNN with 3 iterations, fold 3\n",
      "Train on 22194 samples, validate on 5549 samples\n",
      "Epoch 1/3\n",
      "22194/22194 [==============================] - 1s - loss: 0.0044 - val_loss: 0.0057\n",
      "Epoch 2/3\n",
      "22194/22194 [==============================] - 1s - loss: 0.0043 - val_loss: 0.0052\n",
      "Epoch 3/3\n",
      "22194/22194 [==============================] - 1s - loss: 0.0042 - val_loss: 0.0056\n",
      "Saving training outputs\n",
      "(27743, 198)\n",
      "Saving testing outputs\n",
      "(6936, 198)\n",
      "Training DNN with 3 iterations, fold 4\n",
      "Train on 22194 samples, validate on 5549 samples\n",
      "Epoch 1/3\n",
      "22194/22194 [==============================] - 1s - loss: 0.0041 - val_loss: 0.0053\n",
      "Epoch 2/3\n",
      "22194/22194 [==============================] - 1s - loss: 0.0040 - val_loss: 0.0050\n",
      "Epoch 3/3\n",
      "22194/22194 [==============================] - 1s - loss: 0.0039 - val_loss: 0.0050\n",
      "Saving training outputs\n",
      "(27743, 198)\n",
      "Saving testing outputs\n",
      "(6936, 198)\n",
      "Training DNN with 3 iterations, fold 5\n",
      "Train on 22195 samples, validate on 5549 samples\n",
      "Epoch 1/3\n",
      "22195/22195 [==============================] - 1s - loss: 0.0039 - val_loss: 0.0052\n",
      "Epoch 2/3\n",
      "22195/22195 [==============================] - 1s - loss: 0.0038 - val_loss: 0.0054\n",
      "Epoch 3/3\n",
      "22195/22195 [==============================] - 1s - loss: 0.0037 - val_loss: 0.0054\n",
      "Saving training outputs\n",
      "(27744, 198)\n",
      "Saving testing outputs\n",
      "(6935, 198)\n",
      "Saving Histology blind test outputs\n",
      "(7272, 198)\n",
      "Saving TS04 Reproducibility Results\n",
      "TS04 f1 Saved\n",
      "TS04 f2 Saved\n",
      "Saving TS01 Reproducibility Results\n",
      "TS01 f1 Saved\n",
      "TS01 f2 Saved\n"
     ]
    }
   ],
   "source": [
    "if __name__=='__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep_l",
   "language": "python",
   "name": "deep_l"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
